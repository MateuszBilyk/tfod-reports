---
title: "Report 2"
author: "Mateusz Bi≈Çyk"
date: "`r Sys.Date()`"
output: html_document
---

# Exercise 1
## a)

```{r results='hide'}
x <- seq(-4, 4, length=100)
y <- pnorm(x)
plot(x,y, type = "l", lwd = 2, xlab = "", ylab = "")
df <- c(1,3,5,10,50,100)
y <- sapply(df,function(d) pt(x, d))
apply(y,2,function(y) lines(x,y,col="red"))
```

## b)
We have $T=\frac{\chi_{df}^2-df}{\sqrt{2df}}$ so $P[T \leq t]=P[\chi_{df}^2 \leq t\sqrt{2df}+df]$
```{r results='hide'}
x <- seq(-4, 4, length=100)
y <- pnorm(x)
plot(x,y, type = "l", lwd = 2, xlab = "", ylab = "")
df <- c(1,3,5,10,50,100)
y <- sapply(df,function(d) pchisq(x*sqrt(2*d)+d, d))
apply(y,2,function(y) lines(x,y,col="red"))
```
In both cases we can say that the variables converge in probability to the normal distributed variable.

# Exercise 2
## a)
$P_{H_0}[\overline{X}>\overline{x}]=P_{H_0}[\sum{X_i}>n\overline{x}]=Q(floor(n\overline{x}+1) ,5n)$
Example of graph of p-values for this test for $n=4$:
```{r}

ex2pvalue <- function(n,x_mean){
  1-pgamma(5*n, floor(n*x_mean+1), lower = FALSE)
}
x <- seq(0, 10, length=100)
y <- ex2pvalue(4,x)
plot(x,y, type = "l", lwd = 2, xlab = "", ylab = "")
```

## b)

```{r}
y <- sapply(1:1000,function(a) ex2pvalue(100,mean(rpois(100, 5))) )
hist(y,
main="p-values of 1000 test with n=100",
xlab="p-values",
xlim=c(0,1),
col="darkmagenta",
freq=FALSE
)
```
P-values are more or less uniformly distributed as they should be.

## c)
```{r}

odp <- sapply(1:10^4, function(a)
  {
    y<-sapply(1:1000,function(a) {ex2pvalue(100,mean(rpois(100, 5)))})
    if(min(y)<= 0.05/1000) return(1)
  })
sum(sapply(odp, sum))/10^4
```

```{r}
odp <- sapply(1:10^4, function(a)
  {
    y<-sapply(1:1000,function(a) ex2pvalue(100,mean(rpois(100, 5))))
    if(-sum(2*log(y))>= qchisq(1-0.05, 2*1000))return(1)
  })
sum(sapply(odp, sum))/10^4
```

Computer calculations are not always perfect. Probability of type one error is slightly different because in the Fisher test vector y does not have always perfect uniform distribution which following code proves:
```{r}
odp <- sapply(1:10^4, function(a)
  {
    y<-runif(1000,0,1)
    if(-sum(2*log(y))>= qchisq(1-0.05, 2*1000))return(1)
  })
sum(sapply(odp, sum))/10^4
```

## d) 
Power of the test is probability that when $H_1$ is true we reject $H_0$
- Needle in a haystack
Bonferoni:
```{r}
odp <- sapply(1:10^4, function(a)
  {
    y<-sapply(1:1000,function(a){
        if(a==1){ return(ex2pvalue(100,mean(rpois(100, 7))))}
        if(a>1){return(ex2pvalue(100,mean(rpois(100, 5))))}
      })
    y<-sapply(y,sum)
    
    if(min(y)<= 0.05/1000) return(1)
  })
sum(sapply(odp, sum))/10^4
```

Fisher

```{r}
odp <- sapply(1:10^4, function(a)
  {
    y<-sapply(1:1000,function(a){
        if(a==1){return(ex2pvalue(100,mean(rpois(100, 7))))}
        if(a>1){return(ex2pvalue(100,mean(rpois(100, 5))))}
      })
    y<-sapply(y, sum)
    if(-sum(2*log(y))>= qchisq(1-0.05, 2*1000))return(1)
  })
sum(sapply(odp, sum))/10^4
```
- Many small effects
Bonferoni:
```{r}
odp <- sapply(1:10^4, function(a)
  {
    y<-sapply(1:1000,function(a){
        if(a<=100){ return(ex2pvalue(100,mean(rpois(100, 5.2))))}
        if(a>100){return(ex2pvalue(100,mean(rpois(100, 5))))}
      })
    y<-sapply(y,sum)
    if(min(y)<= 0.05/1000) return(1)
  })
sum(sapply(odp, sum))/10^4
```

Fisher
```{r}
odp <- sapply(1:10^4, function(a)
  {
    y<-sapply(1:1000,function(a){
        if(a<=100){return(ex2pvalue(100,mean(rpois(100, 5.2))))}
        if(a>100){return(ex2pvalue(100,mean(rpois(100, 5))))}
      })
    y<-sapply(y, sum)
    if(-sum(2*log(y))>= qchisq(1-0.05, 2*1000))return(1)
  })
sum(sapply(odp, sum))/10^4
```

# Exercise 3

```{r}
x <- 2:100000
y <- sapply(x,function(a){max(rnorm(a))/sqrt(2*log(a))})
plot(x,y, type = "l", lwd = 2, xlab = "", ylab = "")
y <- sapply(1:10,function(a){
  sapply(x,function(a){max(rnorm(a))/sqrt(2*log(a))})
  })
apply(y,2,function(y) lines(x,y,col="red"))
```

We can see in the graph $R_n$ converges to 1 as n goes to infinity. 

# Exercise 4
## all points
We use theorem that if $X_1,X_2,..,X_n$ are uncorrelated and their combined distribution is normal then they are independent.
Thanks to these theorem we can draw samples independently using rnorm function. 
Under $H_0$
```{r results='hide'}
ex4L <- function(Y){
  n <- length(Y)
  epsilon <- 0.1
  gamma <- (1-epsilon)*sqrt(2*log(n))
  exponential<- function(y) gamma*y-gamma^2/2
  return(1/n*sum(exp(exponential(Y))))
}
ex4Lapprox <- function(Y){
  n <- length(Y)
  epsilon <- 0.1
  gamma <- (1-epsilon)*sqrt(2*log(n))
  exponential <- function(y){
    if(y<sqrt(2*log(n))){return(exp(gamma*y-gamma^2/2))}
    else{return(0)}
  }
  return(1/n*sum(sapply(Y,function(y) exponential(y))))
}
draw_hist <- function(d){
  print(var(d))
  hist(d,
main="",xlab="",
xlim=c(min(d),max(d)),
col="darkmagenta",freq=FALSE)
}
samel<-sapply(c(10^3,10^4,10^5),function(n){
  print(n)
  d<-sapply(1:10^3,function(a) {
    Y<-rnorm(n, 0, 1)
    return (c(ex4L(Y),ex4Lapprox(Y)))})
  draw_hist(d[1,])
  draw_hist(d[2,])
  same<-apply(d,2,function(a){
    if(a[1]==a[2])return(1)
    else return(0)
  })
  return(sum(sapply(same,sum))/10^3)
})
print(samel)

```

Under $H_1$
```{r results='hide'}
ex4L <- function(Y){
  n <- length(Y)
  epsilon <- 0.1
  gamma <- (1-epsilon)*sqrt(2*log(n))
  exponential<- function(y) gamma*y-gamma^2/2
  return(1/n*sum(exp(exponential(Y))))
}
ex4Lapprox <- function(Y){
  n <- length(Y)
  epsilon <- 0.1
  gamma <- (1-epsilon)*sqrt(2*log(n))
  exponential <- function(y){
    if(y<sqrt(2*log(n))){return(exp(gamma*y-gamma^2/2))}
    else{return(0)}
  }
  return(1/n*sum(sapply(Y,function(y) exponential(y))))
}
draw_hist <- function(d){
  hist(d,
main="",xlab="",
xlim=c(min(d),max(d)),
col="darkmagenta",freq=FALSE)
}
sapply(c(10^3,10^4,10^5),function(n){
  d<-sapply(1:10^3,function(a) {
    Y<-rnorm(1, (1-0.1)*sqrt(2*log(n)), 1)
    Y<-append(Y,rnorm(n-1,0,1))
    return (c(ex4L(Y),ex4Lapprox(Y)))})
  draw_hist(d[1,])
  draw_hist(d[2,])
})
```
We know that $L = \tilde{L} \iff \forall_i Y_i<\sqrt{2\log{n}}$.
It follows that $P_{H_0}(L = \tilde{L})=\Phi^n(\sqrt{2\log{n}})$
These values for $n\in \{10^3,10^4,10^5\}$ are about: $0.9,0.915,0.923$


# Exercise 5

```{r results='hide'}
ans<-apply(expand.grid(c(0.05,0.2),c(500,5000,50000)),1,function(a){
  n<-a[2]
  epsi<-a[1]
  crit_value <- sapply(1:10^3,function(i){
    Y<-rnorm(n)
    return(log(ex4L(Y)))
  })  
  sort(sapply(crit_value,sum))
  crit_value<-crit_value[10^3/100*5]
 
  pans<-sapply(1:10^3,function(a){
    Y<-mean(rnorm(10^3, (1+epsi)*sqrt(2*log(n)), 1))
    Y<-append(Y,sapply(1:n-1,function(a) mean(rnorm(10^3,0,1))))
    powers<- c(0,0,0)
    #Neyman-Pearson
    if(log(ex4L(Y))>=crit_value) powers[1] <- 1
    pi_val<-(1-pnorm(10^3*abs(Y),0,sqrt(10^3)))+(pnorm(-10^3*abs(Y),0,sqrt(10^3)))
    #Bonferoni
    if(min(pi_val)<=0.05/n)powers[2] <-1
    #Fisher
    if(-sum(2*log(pi_val))>qchisq(1-0.05, 2*n))powers[3] <-1
    return(powers)
  })
  pans<-apply(pans,1, sum)
  return(pans/10^3)
})
print(ans)
```

